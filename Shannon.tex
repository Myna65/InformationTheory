\section{Les théorèmes de Shannon}

\subsection{Le codage}
	\paragraph{}
	Après avoir étudié le comportement des sources et des canaux théoriques, nous
	allons devoir faire face à quelques réalités plus pratiques. Notamment le fait
	que les messages que nous souhaitons transmettre sont rarement dans le 
	même alphabet que celui d'émission du canal. Afin de remédier à ce problème,
	nous allons maintenant présenter quelques résultats en rapport avec ce 
	changement d'alphabet. 
	
	\paragraph{}
	Par exemple, il est courant de devoir transmettre un signal numérique
	(composé uniquement de 0 et de 1) sur un canal analogique (composé 
	d'un intervalle continu de valeurs), typiquement des ondes radio.
	Un autre exemple est : «Comment représenter des 
	lettres par un signal numérique ?». Et enfin, un problème soulevé par 
	l'apparition d'appareils de communication et par la volonté, vite devenue nécessité, 
	de minimiser la longueur des messages est :
	«En connaissant la répartition des lettres de l'alphabet considéré,
	existe-t-il une transformation optimale de l'alphabet du message
	dans celui du canal et, le cas échéant, existe-t-il un algorithme pour générer cette transformation ?».
	Ces problèmes ont été étudiés afin de créer un alphabet qui minimise la longueur moyenne
	d'une lettre, cela a donné naissance au morse par exemple. 
	
	\paragraph{}
	Étudions le problème d'un point de vue théorique :
	nous appellerons $A_0$, l'alphabet dans lequel nous avons le message à envoyer. Nous avons donc une source 
	$[A_0,\mu]$ que nous souhaitons transmettre par un canal $[A,\nu_x,B]$. Pour cela nous devons encoder
	le message, c'est-à-dire qu'il faut attribuer à toute séquence de $A_0$, une séquence de $A$ afin de 
	pouvoir transmettre le message.

	\paragraph{}
	Ainsi si nous choisissons une séquence

	\[\theta = \dots,\theta_{-1},\theta_{0},\theta_{1},\dots\]

	\paragraph{}
	où les $\theta_i$ sont choisis dans $A_0$. Cette séquence doit être transformée de manière unique en une séquence

	\[x = \dots,x_{-1},x_{0},x_{1},\dots\]

	\paragraph{}
	de A. Cette transformation est appelée le code utilisé pour encoder $A_0$ en $A$. Nous pouvons considérer cette 
	transformation comme le passage dans un canal qui a la propriété remarquable d'être non bruité.
	Nous pouvons en effet associer à toute séquence $\theta$ une séquence $x$ de manière unique. 

	\paragraph{}
	Ces codes ne sont cependant pas pratiques car ils nécessitent de connaître l'intégralité de la séquence, qui est 
	infinie, afin de pouvoir encoder et décoder le message. Nous  nous intéresserons donc aux seuls codes qui peuvent être encodés et 
	décodés à partir d'une séquence 
	finie, on peut trouver une séquence encodée. Pour faire cela, nous découpons la séquence $\theta$ en des sous-séquences finies. 
	Ces sous-séquences peuvent maintenant être vues comme des lettres. Nous choisissons maintenant une fonction d'encodage qui, pour 
	chaque sous-séquence finie de $\theta$, lui associe une sous-séquence finie de $x$. Et nous pouvons transmettre notre message.
	
	\paragraph{}
	Maintenant que nous avons abordé cela rapidement et de manière 
	approximative, nous allons présenter les théorèmes qui sont à la base
	de la théorie de l'information, les théorèmes
	de Shannon, ainsi que leurs preuves.

\subsection{Le premier théorème de Shannon}

	\paragraph{}
	Prenons une source $[A_0,\mu]$ ergodique et un canal $[A,\nu_x,B]$ stationnaire non-anticipatif de mémoire finie $m$. 
	Soit $H_0$, l'entropie de la source et $C$, la capacité du canal. Le théorème s'applique si et seulement si la capacité 
	du canal est supérieure à l'entropie de la source. Nous pouvons donc prendre un nombre $\lambda$ tel que $\lambda<\frac{1}{2}(C-H_0)$. 
	
	\paragraph{}
	Comme annoncé précédemment, nous allons découper la séquence infinie en sous-séquence finie de longueur infinie en des sous-séqences 
	de longueur $n$ finie. Nous avons vu dans les chapitres précédents des propriétés de ces sous-séqences finies. Nous allons les appliquer ici.

	\paragraph{}
	Commençons par parler de la source, elle est ergodique et donc elle a la propriété E (voir \ref{E_prop}). 
	Et donc si la longueur $n$ de la séquence grandit, les séquences $\alpha$ de l'alphabet	$A_0$ peuvent se 
	séparer en deux groupes, un groupe de «haute» probabilité dans lequel chaque séquence $\alpha$ est telle que
	
	\[
		\begin{array}{crcl}
			&\frac{\lg \mu\left(\alpha\right)}{n}+H_0 &>& -\lambda\\
			\Leftrightarrow & \mu(\alpha) & > &2^{-n\left(H_0+\lambda\right)}\\
		\end{array}
	\]
	
	\paragraph{}
	Et en un groupe de basse probabilité dont la probabilité totale peut être réduite autant que possible.

	\paragraph{}
	Vu que la probabilité totale est 1, le nombre de séquences dans le groupe de haute probabilité est inférieur à $2^{n(H_0+\lambda)}<2^{n(C-\lambda)}$.
	Nous noterons ces séquences $\alpha_1,\alpha_2,\dots$, et l'ensemble des séquences de celui à basse probabilité $\alpha_0$.
	
	\paragraph{}
	Parlons maintenant du canal, nous pouvons dire, grâce au lemme fondamental de Feinstein (voir \ref{Feinstein_lemma}) 
	que pour un $n$ suffisamment grand, il existe un groupe $\{u_i\}$ de taille $N > 2^{n(C-\lambda)}$ de séquences $u$ de longueur $n+m$ de lettres 
	de $A$.
	
	\paragraph{}
	Il existe donc d'avantage de membres dans ce groupe que de séquences $\alpha_i$. Nous pouvons donc associer à chaque séquence $\alpha_i$
	un séquence $u_i$ (clarifier la relation). Associons les séquences inutilisées (il en reste au moins une) de $\{u_i\}$ à $\alpha_0$.
	
	\paragraph{}
	Nous avons donc associé à chaque séquence $\alpha$ de taille $n$ contenant des lettres de l'alphabet $A_0$ une séquence $u$ de taille $n+m$ de
	lettres de $A$ qui appartient au groupe $\{u_i\}$. 
	
	\paragraph{}
	Il ne reste plus qu'à diviser $\theta$ en séquences de longueur $n$ cette séquence est du type $\alpha$, on peut donc lui 
	associer une séquence du type $u$ de longueur $n+m$. En utilisant ces séquences, on peut construire un séquence $x$ de $A$. On a donc bien transformé
	un séquence de $A_0$ en une séquence de $A$. (préciser la limite à l'infini)
	
	\paragraph{}Nous avons discuté plus haut du fait que nous pouvons voir le changement d'alphabet comme le passage dans un canal non-bruité. 
	Si nous chainons (changer ce mot) ces canaux, nous pouvons en trouver un de la forme $[A_0,\lambda_0,B]$ qui envoie directement $A_0$ dans $B$ 
	sans énoncer explicitement le passage par $A$. Néanmoins $\lambda_0$ n'est pas défini clairement, il s'agit de l'exprimer à partir de $\nu_x$ et
	du changement de variable $x(\theta)$ (changer ce nom pourri et trouver une place pour définir cette relation plus haut). 
	Vu que la séquence $x$ introduite dans le canal bruitée est obtenue à partir de $x(\theta)$, on trouve facilement $\lambda_0=\nu_{x(\theta)}$.
	
	\paragraph{}
	(Déplacer) Maintenant que nous allons nous intéresser de plus près aux séquences de $B$, nous allons les nommer dans un souci
	de clarté $\beta_i$.
	
	\paragraph{}
	Nous allons maitenant réintroduire la notation composée. Rappelons que dans cette notation nous calculons la probabilité d'avoir la séquence
	d'entrée $\alpha_i$ et la séquence de sortie $\beta_i$ sans aucune hypothèse sur ces deux-ci. On pose $C = A_0 \times B$ et on a donc
	
	
	\[\omega(\alpha_i\times\beta_k)=\int\limits_{\alpha_i}\lambda_0(\beta_k)d\mu(\theta)=\int\limits_{\alpha_i}\nu_{x(\theta)}(\beta_k)d\mu(\theta)\]
	où $\omega$ est la probabilité d'avoir un élément de $C$.
	
	\paragraph{}
	Mais si $\theta \in \alpha_i$, alors $x(\theta) \in u_i$ (préciser le 0), et vu que $\nu_x(\beta_k)$ prend la même valeur
	pour tous les $x \in u_i$, nous écrirons donc cette expression sous la forme $\nu_{u_i}(\beta_k)$ et de suite on trouve $\nu_{x(\theta)}=\nu_{u_i}(\beta_k)$.
	On remplaçant cette valeur dans l'équation précédente, on trouve
	
	\[\omega(\alpha_i\times\beta_k)=\mu(\alpha_i)\nu_{u_i}(\beta_k)\]
	
	\paragraph{} (Déplacer ?)
	Rappelons quand même la définition de $\alpha_i \times \beta_i$, celà dénote la probabilité d'avoir à la fois l'élement $\alpha_i$ et l'élément $\beta_i$ c'est-à-dire
	\begin{enumerate}
		\item Il faut que la séquence émise par la source $[A_0,\mu]$ soit $\alpha_i$ (préciser le cas 0).
		\item Et aussi que, après la transmittion de cette séquence dans le canal $[A_0,\lambda_0,B]$, on obtienne bien une séquence de $n+m$ lettres dont les $n$ 
		dernières sont bien la séquence $\beta_k$
	\end{enumerate} 
	
	\paragraph{}
	À partir de maintenant, nous allons choisir une certaine séquence $\beta_k$.
	Prenons la valeur de $i$ qui maximisie $\omega(\alpha_i,\beta_k)$, nous appelerons cette valeur $i_k$ (préciser l'égalité).
	Par la formule des probabilité conditionnelles, on trouve que le probabilité d'obtenir $\alpha_i$ sachant $\beta_k$ est de 
	
	\[\frac{\omega(\alpha_i \times \beta_k)}{\omega(A_0^I \times \beta_k)}\]
	
	\paragraph{}
	Le dénominateur n'a pas de dépendance ne $i$ et donc nous pouvons déduire facilement que $\alpha_{i_k}$ est la séquence $\alpha_i$ la plus probable pour un $\beta_k$ donné.
	Notons
	
	\[\sum_k\sum_{i\neq i_k}\omega(\alpha_i \times \beta_k) = P.\]
	
	\paragraph{}
	$P$ est donc la probabilité que après transmission dans le canal $[A_0,\lambda_0,B]$ nous trouvions un séquence $\beta_k$ et que celle-ci ne soit pas engendrée par 
	la séquence source $\alpha_{i_k}$, c'est-à-dire par la séquence la plus probable. Nous avons donc ici la probabilité que en recevant une séquence donnée, on en déduise
	qu'elle a été émise par une certaine séquence $\alpha_{i_k}$ alors que ce n'est pas le cas. (petite phrase en plus ?)
	
	\paragraph{}
	Les séquences $u_i$ dans lesquelles nous encodons les séquences $\alpha_i$, forment un groupe. Nous pouvons donc en déduire l'existence d'un groupe $\{B_i\}$, dont 
	chaque élément est composé de certaines séquences $\beta_k$, tel que 
	\begin{enumerate}
		\item $\nu_{u_i}(B_i) > 1-\lambda$ (?).
		\item $B_i$ et $B_j$ n'aient aucune séquence commune si $i\neq j$
	\end{enumerate}
	
	\paragraph{}Si nous prenons la somme sur toutes les valeurs de $\beta_k$, on trouve
	
	\[\omega(\alpha_i \times B_i)=\mu(\alpha_i)\nu_{u_i}(B_i)>(1-\lambda)\mu(\alpha_i).\]
	
	\paragraph{}
	Et donc on trouve que la probabilité d'obtenir $\beta_k$ en sortie sachant que nous avons $\alpha_i$ en entrée est
	
	\[P_{\alpha_i}(B_i)=\frac{\omega(\alpha_i \times B_i)}{\mu(\alpha_i)}>1-\lambda.\]
	
	\paragraph{}
	Donc le système des $\alpha_i$ ainsi que celui des $\beta_k$ satisfont aux conditions de la deuxième inégalités fondamentale (voir \ref{second_inequality}). 
	Et donc nous pouvons en déduire que
	
	\[P\le \lambda\]
	
	\paragraph{}
	Ce résultat est connu sous le nom de premier thérorème de Shannon que nous pouvons formuler de manière plus synthétique de cette façon :
	
	\paragraph{Théorème}\ 
	\newline
	Soit 
	\begin{enumerate}
		\item un canal stationnaire, non-anticipatif $[A,\nu_x,B]$ d'une capacité ergodique $C$ et d'une mémoire finie $m$
		\item une source ergodique $[A_0,\mu]$ d'une entropie $H_0<C$.
	\end{enumerate}
	Alors pour tout $n$ suffisamment grand, on a que le sortie de la source $[A_0,\mu]$ peu être encodée dans l'alphabet $A$ de telle sorte que
	chaque séquence $\alpha_i$ de $n$ lettres de l'alphabet $A_0$ puisse être encodée en une séquence $u_i$ de $n+m$ caractères de l'alphabet $A$. 
	Et que, si cette séquence était transmisse à travers le canal donné, alors on peut déterminer la séquence $\alpha_i$ transmise avec une probabilité 
	supérieure à $1-\varepsilon$ à partir de la séquence obtenue à la sortie du canal.



