
\section{Le codage}
	\paragraph{}
	Après avoir étudié le comportement des sources et des canaux théoriques, nous
	allons devoir faire face à quelques réalités plus pratiques. Notamment le fait
	que les messages que nous souhaitons transmettre sont rarement dans le 
	même alphabet que celui d'émission du canal. Afin de palier à ce problème,
	nous allons maintenant présenter quelques résultats en rapport avec ce 
	changement d'alphabet. 
	
	\paragraph{}
	Par exemple, il est courant de devoir transmettre un signal numérique
	(composé uniquement de 0 et de 1) sur un canal analogique (composé 
	d'un intervalle continu de valeurs), typiquement des ondes radio.
	Un autre exemple est : «Comment représenter des 
	lettres par un signal numérique ?». Et enfin, un problème soulevé par 
	l'apparition d'appareils de communication et la volonté, vite devenue nécessité, 
	de minimiser la longueur des messages est
	«En connaissant la répartition des lettres de l'alphabet considéré,
	existe-t-il un transformation optimale de l'alphabet du message
	dans celui du canal et, le cas échéant, existe-t-il un algorithme pour générer cette transformation ?».
	Ces problèmes ont été étudiés afin de créer un alphabet qui minimise la longueur moyenne
	d'une lettre, cela a donné naissance au morse par exemple. 
	
	\paragraph{}
	Étudions le problème d'un point de vue théorique :
	nous appellerons $A_0$, l'alphabet dans lequel nous avons le message. Nous avons donc un source 
	$[A_0,\mu]$ que nous souhaitons transmettre sur un canal $[A,\nu_x,B]$. Pour cela nous devons encoder
	le message, c'est-à-dire qu'il faut attribuer à toute séquence de $A_0$, une séquence de $A$ afin de 
	pouvoir transmettre le message.

	\paragraph{}
	Ainsi si nous choisissons un séquence

	\[\theta = \dots,\theta_{-1},\theta_{0},\theta_{1},\dots\]

	\paragraph{}
	où les $\theta_i$ sont choisis dans $A_0$. Cette séquence doit être transformée de manière unique en une séquence

	\[x = \dots,x_{-1},x_{0},x_{1},\dots\]

	\paragraph{}
	de A. Cette transformation est appelée le code utilisé pour encoder $A_0$ en $A$. Nous pouvons considérer cette 
	transformation comme le passage dans un canal qui a la propriété remarquable d'être non bruité.
	Nous pouvons en effet associer à toute séquence $\theta$ un séquence $x$ de manière unique. 

	\paragraph{}
	Ces codes ne sont cependant pas pratiques car ils nécessitent de connaître l'intégralité de la séquence, qui est 
	infinie, afin de pouvoir encoder et décoder le message. Nous  nous intéresserons donc aux seuls codes qui peuvent être encodés et 
	décodés à partir d'une séquence 
	finie, on peut trouver une séquence encodée. Pour faire cela, nous découpons la séquence $\theta$ en des sous-séquences finies. 
	Ces sous-séquences peuvent maintenant être vues comme des lettres. Nous choisissons maintenant une fonction d'encodage qui pour 
	chaque sous-séquence finie de $\theta$ lui associe une sous-séquence finie de $x$. Et nous pouvons transmettre notre message.
	
	\paragraph{}
	Maintenant que nous avons abordé cela rapidement et de manière 
	approximative, nous allons présenter les théorèmes qui sont à la base
	de la théorie de l'information, les théorèmes
	de Shannon, ainsi que leurs preuves.

\section{Le premier théorème de Shannon}

	\paragraph{}
	Prenons une source $[A_0,\mu]$ ergodique et un canal $[A,\nu_x,B]$ stationnaire non-anticipatif de mémoire finie $m$. 
	Soit $H_0$, l'entropie de la source et $C$, la capacité du canal. Le théorème s'applique si et seulement si la capacité 
	du canal est supérieure à l'entropie de la source. Nous pouvons donc prendre un nombre $\lambda$ tel que $\lambda<\frac{1}{2}(C-H_0)$. 

	\paragraph{}
	Vu que la source est ergodique, elle la propriété E (voir \ref{E_prop}). Et donc, pour un $n$ grand (?), les séquences $\alpha$ de l'alphabet 
	$A_0$ peuvent se séparer en deux groupes, un groupe de «haute» probabilité dans lequel chaque séquence $\alpha$ est telle que (?)
	
	\[
		\begin{array}{crcl}
			&\frac{\lg \mu\left(\alpha\right)}{n}+H_0 &>& -\lambda\\
			\Leftrightarrow & \mu(\alpha) & > &2^{-n\left(H_0+\lambda\right)}\\
		\end{array}
	\]
	
	\paragraph{}
	Et en un groupe de basse probabilité dont la probabilité totale peut être réduite autant que possible.

	\paragraph{}
	Vu que la probabilité totale est 1, le nombre de séquences dans le groupe de haute probabilité est inférieur à $2^{n(H_0+\lambda)}<2^{n(C-\lambda)}$.
	Nous noterons ces séquences $\alpha_1,\alpha_2,\dots$, et l'ensemble des séquences de celui à basse probabilité $\alpha_0$.
	
	\paragraph{}
	Quant au canal, nous pouvons dire par le lemme fondamental de Feinstein (voir \ref{Feinstein_lemma}) nous savons que pour un $n$ suffisamment grand (?), 
	il existe un groupe $\{u_i\}$ de séquences $u$ de longueur $n+m$ de lettres de $A$ de taille $N > 2^{n(C-\lambda)}$.
	
	\paragraph{}
	Il existe donc d'avantage de membres dans ce groupe que de séquences $\alpha_i$. Nous pouvons donc associer à chaque séquence $\alpha_i$
	un séquence $u_i$ (clarifier la relation). Associons les séquences inutilisées (il en reste au moins une) de $u_i$ à $\alpha_0$.
	
	\paragraph{}
	Nous avons donc associé à chaque séquence $\alpha$ de taille $n$ contenant des lettres de l'alphabet $A_0$ une séquence $u$ de taille $n+m$ de
	lettres de $A$ qui appartient au groupe $\{u_i\}$. 
	
	\paragraph{}
	Il ne reste plus qu'à diviser $\theta$ en séquences de longueur $n$ cette séquence est du type $\alpha$, on peut donc lui 
	associer une séquence du type $u$ de longueur $n+m$. En utilisant ces séquences, on peut construire un séquence $x$ de $A$. On a donc bien transformé
	un séquence de $A_0$ en une séquence de $A$. (préciser la limite à l'infini)
	
	\paragraph{}
	Nous pouvons donc passer dans l'espace composé $[C,\omega]$ tel que défini plus haut (reste à faire). Notons par $\beta_k$ les séquences de longueur $n$ de $B$.
	Par la définition de la distribution $\omega$, on a 
	
	\[\omega(\alpha_i\times\beta_k)=\int\limits_{\alpha_i}\lambda_0(\beta_k)d\mu(\theta)=\int\limits_{\alpha_i}\nu_{x(\theta)}(\beta_k)d\mu(\theta)\]
	
	\paragraph{Théorème}\ 
	\newline
	Soit 
	\begin{enumerate}
		\item un canal stationnaire, non-anticipatif $[A,\nu_x,B]$ d'une capacité ergodique $C$ et d'une mémoire finie $m$
		\item une source ergodique $[A_0,\mu]$ d'une entropie $H_0<C$.
	\end{enumerate}
	Alors pour tout $n$ suffisamment grand, on a que le sortie de la source $[A_0,\mu]$ peu être encodée dans l'alphabet $A$ de telle sorte que
	chaque séquence $\alpha_i$ de $n$ lettres de l'alphabet $A_0$ puisse être encodée en une séquence $u_i$ de $n+m$ caractères de l'alphabet $A$. 
	Et que, si cette séquence était transmisse à travers le canal donné, alors on peut déterminer la séquence $\alpha_i$ transmise avec une probabilité 
	supérieure à $1-\varepsilon$ à partir de la séquence obtenue à la sortie du canal.


